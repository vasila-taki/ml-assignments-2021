{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEawnqE8J5Bc"
   },
   "source": [
    "### This Jupyter file is structured as:\n",
    "* Problem 2 - Question 2a-3a\n",
    "* Problem 2 - Question 2b-3b\n",
    "* Problem 2 - Question 2c-3c\n",
    "* Problem 2 - Question 2d-3d\n",
    "* Results and Comments for both Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSsqsDAmPLxk"
   },
   "source": [
    "## Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1641848787315,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "cSX0K_145Xkn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.core.defchararray import mod\n",
    "from sklearn.model_selection import KFold \n",
    "import math \n",
    "from numpy.ma.core import power\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3MDqbbHKJJZ"
   },
   "source": [
    "\n",
    "## Functions used in Problems 2,3 - Questions 2a, 3a, 2b, 3b, 2c, 3c, 2d, 3d  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1641848787318,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "YTMFdseCPWFu"
   },
   "outputs": [],
   "source": [
    "def calculate_AIC(k, L):\n",
    "  \"\"\"\n",
    "    AIC criterion for selecting a model. \n",
    "    For the Pima Indians Diabetes Database: n=768 samples with k=8 features -> n/k=96 and we don't need to choose the second order criterion.\n",
    "  \"\"\"\n",
    "  aic = (-2) * L + 2 * k\n",
    "\n",
    "  return aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1641848787321,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "ANd_2RX7WPLP"
   },
   "outputs": [],
   "source": [
    "def calculate_BIC(k, L, N):\n",
    "  \"\"\"\n",
    "    BIC criterion for criterion for selecting a model based on information theory set within a Bayesian context. \n",
    "  \"\"\"\n",
    "  bic = (-2) * L + k * np.log(N)\n",
    "\n",
    "  return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1641848787324,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "s964ujGyFUNO"
   },
   "outputs": [],
   "source": [
    "# Vectors collecting the results of the AIC, BIC criterion of classes 0 and 1 on entire dataset + collecting \n",
    "# average accuracies from Bayes classifiers.\n",
    "AIC_0 = []\n",
    "AIC_1 = []\n",
    "BIC_0 = []\n",
    "BIC_1 = []\n",
    "average_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKojFzCNK4wS"
   },
   "source": [
    "## Dataset upload and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1641848816177,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "ExJUCHO85g2q"
   },
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "df = pd.read_csv(\"./UCIdata-exercise1/pima-indians-diabetes.data\", header= None)\n",
    "\n",
    "np_A = df.values\n",
    "np.take(np_A,np.random.permutation(np_A.shape[0]),axis=0,out=np_A)\n",
    "\n",
    "np_X = np_A[:, 0:8]  \n",
    "Y = np_A[:, [8]]\n",
    "Y = np.array(Y, dtype=np.int64)\n",
    "\n",
    "X_0 = []\n",
    "X_1 = []\n",
    "\n",
    "\n",
    "for i, value in enumerate(np_X):\n",
    "  if(Y[i] == 0):\n",
    "    X_0.append(value)\n",
    "  elif(Y[i] == 1):\n",
    "    X_1.append(value)\n",
    "\n",
    "X_0 = np.array(X_0, dtype=np.float64)\n",
    "X_1 = np.array(X_1, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKmZi4UOMNab"
   },
   "source": [
    "## Functions used in Problem 2 - Question 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1641848872022,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "fkk_hHs8SFfN"
   },
   "outputs": [],
   "source": [
    "def calculate_theta_2a(X):\n",
    "  \"\"\"\n",
    "   Calculates mean value theta_ML of the distribution by the equation described in the next section.\n",
    "  \"\"\"\n",
    "  x1 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x1 = x1 + X[i]\n",
    "  \n",
    "  theta = (1 / len(X)) * x1\n",
    "  theta = np.reshape(theta,(len(X[0]),1))\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1641848880677,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "GnPwSakfOwHq"
   },
   "outputs": [],
   "source": [
    "def calculate_a_2a(X, theta):\n",
    "  \"\"\"\n",
    "   Calculates variance a of covariance matrix Sigma = a*identity of the distribution by the equation described in the next section..\n",
    "  \"\"\"\n",
    "  x3 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x1 = X[i] - theta.T\n",
    "    x2 = np.matmul(x1, x1.T)\n",
    "    x3 = x3 + x2\n",
    "  \n",
    "  a = (2 / (len(X) * len(X[0]))) * x3\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641848883383,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "RY1M85T1SZb_"
   },
   "outputs": [],
   "source": [
    "def calculate_log_likelihood_2a(X, theta, a):\n",
    "  \"\"\"\n",
    "    Calculates log likelihood of the distribution given the assumptions we made about its form.\n",
    "  \"\"\"\n",
    "  x1 = (-(len(X) * len(X[0])) / 2) * np.log(2 * math.pi)\n",
    "  x2 = (-(len(X) * len(X[0]))/4) * np.log(a)\n",
    "  x3 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x4 = X[i] - theta.T\n",
    "    x5 = np.matmul(x4, x4.T)\n",
    "    x3 = x3 + x5\n",
    "\n",
    "  x3 = (-1/(2 * a)) * x3\n",
    "\n",
    "  L = x1 + x2 + x3\n",
    "\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmNJvOBqOwwP"
   },
   "source": [
    "## Problem 2 - Question 2a\n",
    "\n",
    "What we want to do is obtain an estimate for the probability density functions (pdfs) of our 2 classes. \n",
    "\n",
    "What we have is our training set (where we consider all samples independent one from another), and the assumption that the distribution of our pdf's is guassian, with covariance matrices that are diagonal, and all of those diagonal elements are equal. So we consider that $p(\\vec{x})$ is known, and we want to find estimates for the unknown parameter vector $\\vec{θ}$ and the variance.\n",
    "\n",
    "$p(\\vec{x}) ≡ p(\\vec{x}|\\vec{θ})$, and so the likelihood of $\\vec{\\theta}$ with respect to $X$ is:\n",
    "\n",
    " $p(X|\\vec{θ}) ≡ p(\\vec{x_{1}},...,\\vec{x_{N}}|\\vec{\\theta}) ≡ p(\\vec{x}|{\\theta}) = \\prod_{k=1}^N p(\\vec{x_{k}}|\\vec{\\theta})$\n",
    "\n",
    "In order to find our estimates, we are going to implement the Maximum Likelihood method (ML), according to which we calculate them by maximizing the log likelihood with respect to $\\vec{\\theta}$ and $Σ$.\n",
    "\n",
    "$p(\\vec{x_k})=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}exp(-\\frac{1}{2}(\\vec{x_k}-\\vec{\\theta})^T\\Sigma^{-1}(\\vec{x_k}-\\vec{\\theta}))=p(\\vec{x_k}|\\vec{\\theta},\\Sigma)$\n",
    "\n",
    "Log likelihood: $\\Pi(\\vec{\\theta},\\Sigma)=ln(\\prod_{k=1}^N p(\\vec{x_{k}}|\\vec{\\theta}))=\n",
    "-Nln((2\\pi)^{\\frac{n}{2}})-Nln(|\\Sigma|^{\\frac{1}{2}})-\\frac{1}{2}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^T\\Sigma^{-1}(\\vec{x_k}-\\vec{\\theta})$\n",
    "\n",
    "Here $\\Sigma=aI$ and $det(I)=1 \\rightarrow det(aI)=a^n$ which leads to $|\\Sigma|^{\\frac{1}{2}}=a^{\\frac{n}{2}}$ and $\\Sigma^{-1}\\Sigma=\\Sigma^{-1}aI \\rightarrow \\Sigma^{-1}=\\frac{1}{a}I$.\n",
    "\n",
    "\n",
    "After substituting $\\Sigma^{-1}$ the log-likelihood becomes: $\\Pi(\\vec{\\theta},aI)=-Nln((2\\pi)^{\\frac{n}{2}})-Nln((a^{\\frac{n}{2}})^{\\frac{1}{2}})-\\frac{1}{2}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^T\\frac{1}{a}I(\\vec{x_k}-\\vec{\\theta})$\n",
    "\n",
    "Afterwards, we take the partial derivatives with respect to $a$ and $\\vec{\\theta}$:\n",
    "* $\\frac{\\partial \\Pi}{\\partial a}=0 \n",
    "\\rightarrow 0 - \\frac{Nn}{4}\\frac{1}{a}+\\frac{1}{2a^2}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^TI(\\vec{x_k}-\\vec{\\theta}) = 0 \\rightarrow  \\frac{Nn}{4}\\frac{1}{a}+\\frac{1}{2a^2} =\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^TI(\\vec{x_k}-\\vec{\\theta}) \n",
    "\\rightarrow a=\\frac{2}{Nn}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^TI(\\vec{x_k}-\\vec{\\theta}) \\rightarrow\n",
    "a=\\frac{2}{Nn}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^2$\n",
    "\n",
    "* $\\frac{\\partial \\Pi}{\\partial \\vec{\\theta}} = 0 \\rightarrow\n",
    "\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})=0 \\rightarrow \n",
    "\\sum_{k=1}^N\\vec{x_k}=N\\vec{\\theta} \\rightarrow \\vec{\\theta_{ML}}=\\frac{1}{N}\\sum_{k=1}^N\\vec{x_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CA1ntoQkCmL"
   },
   "source": [
    "So we have splitted our whole dataset in two parts, according to the class each of the samples belongs to, and we are going to plug those 2 sub-datasets into the equations we derived, in order to obtain our calculations for the estimates of the mean and the variance of the 2 pdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1641848888645,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "k7wDT7SwiFBR",
    "outputId": "257c1459-a0f8-4255-866e-c7cca45e74ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of mean for class 0: \n",
      "[[  3.298   ]\n",
      " [109.98    ]\n",
      " [ 68.184   ]\n",
      " [ 19.664   ]\n",
      " [ 68.792   ]\n",
      " [ 30.3042  ]\n",
      " [  0.429734]\n",
      " [ 31.19    ]]\n",
      "Estimation of variance for class 0: [[2796.93338887]]\n"
     ]
    }
   ],
   "source": [
    "# Estimates for class 0\n",
    "theta_ml_0 = calculate_theta_2a(X_0)\n",
    "a_0 = calculate_a_2a(X_0, theta_ml_0)\n",
    "\n",
    "print(f\"Estimation of mean for class 0: \\n{theta_ml_0}\")\n",
    "print(f\"Estimation of variance for class 0: {a_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1641848892373,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "dD-04eQ6MjMz",
    "outputId": "099e8677-23fb-41f1-a45e-6daefd4a1744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of mean for class 1:\n",
      " [[  4.86567164]\n",
      " [141.25746269]\n",
      " [ 70.82462687]\n",
      " [ 22.1641791 ]\n",
      " [100.3358209 ]\n",
      " [ 35.14253731]\n",
      " [  0.5505    ]\n",
      " [ 37.06716418]]\n",
      "Estimation of variance for class 1: [[5284.32644968]]\n"
     ]
    }
   ],
   "source": [
    "# Estimates for class 1\n",
    "theta_ml_1= calculate_theta_2a(X_1)\n",
    "a_1 = calculate_a_2a(X_1, theta_ml_1)\n",
    "\n",
    "print(f\"Estimation of mean for class 1:\\n {theta_ml_1}\")\n",
    "print(f\"Estimation of variance for class 1: {a_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE84kH9Vpyz2"
   },
   "source": [
    "In order to assess the goodness of our models for each class, we are going to  calculate the (AIC) and the (BIC) criterions:\n",
    "\n",
    "$(AIC) = -2log(L(\\vec{Θ})) + 2k$, \n",
    "\n",
    "where: $L(\\theta)$ is the  likelihood  of  the  candidate  model  given  the  data  when  evaluated at the maximum likelihood estimate of $\\vec{θ}$, and $k$ the number of estimated parameters in the candidate model (2 in our cases).\n",
    "So, a model with a high value for the likelihhod function scores a low AIC.\n",
    "Models with large numbers of parameters tend to score high values of the $L(\\vec{\\theta}) $function, but theres a drawback to that: lots of parameters increase the complexity of the model. And that exactly is depicted by the term \"$+2k$\", that is used to penalise such models. \n",
    "\n",
    "Also, it is kind of pointless to calculate AIC only for one model. AIC is calculated when we want to compare models, and the “best” model is the candidate with the smallest AIC. \n",
    "\n",
    "As for the BIC criterion, it is calculated by the formula:\n",
    "$(BIC) = -2log(L(\\vec{\\theta})) + klogn$\n",
    "\n",
    "where $n$ is the number of observations.\n",
    "\n",
    "The difference between BIC and the AIC is the greater penalty imposed on the term that has to do with the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1641848953127,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "BtD8Z7R5R2gj"
   },
   "outputs": [],
   "source": [
    "# Finding the log likelihoods of each class and calculating AIC and BIC criterion.\n",
    "k = 2\n",
    "log_likelihood_1 = calculate_log_likelihood_2a(X_1, theta_ml_1, a_1)\n",
    "\n",
    "log_likelihood_0 = calculate_log_likelihood_2a(X_0, theta_ml_0, a_0)\n",
    "\n",
    "aic_1 = calculate_AIC(k, log_likelihood_1)\n",
    "bic_1 = calculate_BIC(k, log_likelihood_1, len(X_1))\n",
    "\n",
    "aic_0 = calculate_AIC(k, log_likelihood_0)\n",
    "bic_0 = calculate_BIC(k, log_likelihood_0, len(X_0))\n",
    "\n",
    "AIC_0.append(aic_0)\n",
    "AIC_1.append(aic_1)\n",
    "BIC_0.append(bic_0)\n",
    "BIC_1.append(bic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYgi4y0Hnlw7"
   },
   "source": [
    "## Functions used in Problem 3 - Question 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1641848962584,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "RUOl7-DrQtE-"
   },
   "outputs": [],
   "source": [
    "def bayes_classifier_3a(x, th_1, th_0):\n",
    "  \"\"\"\n",
    "    Classifies points x to classes: 1 (with its distribution having mean th_1) and 0 (with its distribution having mean th_0).\n",
    "    The classification is done by calculating the Euclidean distance ||x-mean_value|| between the point x and the classes.\n",
    "  \"\"\"\n",
    "  # Class 1\n",
    "  a = np.linalg.norm(th_1 - x)\n",
    "  # Class 0\n",
    "  b = np.linalg.norm(th_0 - x)\n",
    "\n",
    "  if a > b:\n",
    "    # Euclidean distance of x is smaller to class 0 than to class 1.\n",
    "    return 0\n",
    "  else:\n",
    "    # Euclidean distance of x is smaller to class 1 than to class 0.\n",
    "    return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641848964658,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "7ToWyUyclaT9"
   },
   "outputs": [],
   "source": [
    "def produce_y_pred_3a(X_tst, th_1, th_0):\n",
    "    \"\"\"\n",
    "     Performs classification using the Bayes classifier to the test set and collects the\n",
    "     results to a vector.\n",
    "    \"\"\"\n",
    "    Y_pred = []\n",
    "\n",
    "    for i in range(len(X_tst)):\n",
    "      Y_pred.append(bayes_classifier_3a(X_tst[i], th_1, th_0))\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641848966068,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "Hc_xtba9khAv"
   },
   "outputs": [],
   "source": [
    "def cross_validation_3a(k):\n",
    "  \"\"\"\n",
    "    Performs k-fold validation in the dataset, finding the average error between all folds in order to give a not biased view \n",
    "    of the performance of the classifier.\n",
    "  \"\"\"\n",
    "  accu = []\n",
    "\n",
    "  kf = KFold(n_splits=k, random_state=None)\n",
    "  \n",
    "  for i , j in kf.split(np_X):\n",
    "    X_train , X_test = np_X[i,:], np_X[j,:]\n",
    "    y_train , y_test = Y[i], Y[j]\n",
    "\n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "\n",
    "    for i, value in enumerate(X_train):\n",
    "      if y_train[i] == 1:\n",
    "        X_1.append(value)\n",
    "      else:\n",
    "        X_0.append(value)\n",
    "\n",
    "    theta_ml_1= calculate_theta_2a(X_1)\n",
    "\n",
    "    theta_ml_0= calculate_theta_2a(X_0)\n",
    "\n",
    "    y_pred = produce_y_pred_3a(X_test, theta_ml_1, theta_ml_0)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accu.append(acc)\n",
    "     \n",
    "  avg_acc = sum(accu)/k\n",
    "  \n",
    "  return avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXFmZT_CIQtf"
   },
   "source": [
    "## Problem 3 - Question 3a\n",
    "\n",
    "A Bayes classifier assigns $\\vec{x}$ to $\\omega_1$ if:\n",
    " $P(\\omega_1|\\vec{x})>P(\\omega_2|\\vec{x}) $ \n",
    " and to $\\omega_2$ if $P(\\omega_1|\\vec{x}) < P(\\omega_2|\\vec{x}) $.\n",
    "\n",
    "For equiprobable classes the test becomes: $p(\\vec{x}|\\omega_1)>p(\\vec{x}|\\omega_2)$ and $p(\\vec{x}|\\omega_1)< p(\\vec{x}|\\omega_2)$ respectively.\n",
    "\n",
    "* In the case of $\\Sigma=\\sigma^2I$, (according to the calculations we did in the lectures), we concluded that we can assign class $\\omega_1$ if the Euclidean distance of $\\vec{x}$ to $\\omega_1$ is smaller than the respective distance to $\\omega_2$:\n",
    "  $\\vec{x} \\rightarrow \\omega_1: ||\\vec{x}-\\vec{\\mu_{1}}||<||\\vec{x}-\\vec{\\mu_{2}}||$\n",
    "\n",
    "In this question, the Bayes classifier we designed calculates the distances between the query vector we want to classify, and the estimations of the means of the pdfs that correspond to each one of our classes, which we have calculated using the ML method.\n",
    "\n",
    "In order to calculate the accuracy of our classifier, we are going to implement K-fold cross validation for K = 10. As we have mentioned before, at each iteration of the method, we split our dataset into train and test sets. Now, we use the train set in order to estimate the parameters of each of the pdfs of our classes, and the test set to test the estimated model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1641848971229,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "HovQij12ojWA"
   },
   "outputs": [],
   "source": [
    "#obtain average accuracy from Kfold (for K = 10)\n",
    "avg_acc_3a = cross_validation_3a(10)\n",
    "\n",
    "average_accuracy.append(avg_acc_3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1Cgc-4sJgTY"
   },
   "source": [
    "## Functions used in Problem 2 - Question 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1641848973690,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "MunryE8G5u_t"
   },
   "outputs": [],
   "source": [
    "def calculate_theta_2b(X):\n",
    "  \"\"\"\n",
    "   Calculates mean value theta_ML of the distribution by the equation described in the next section.\n",
    "  \"\"\"\n",
    "  x1 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x1 = x1 + X[i]\n",
    "  \n",
    "  theta = (1 / len(X)) * x1\n",
    "  theta = np.reshape(theta,((len(X[0]),1)))\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1641848976502,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "7yRpbsIbScCb"
   },
   "outputs": [],
   "source": [
    "def calculate_covariance_2b(X, theta):\n",
    "  \"\"\"\n",
    "   Calculates covariance matrix of the distribution by the equation described in the next section.\n",
    "  \"\"\"\n",
    "  x3 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x1 = X[i] - theta.T\n",
    "    x2 = np.matmul(x1.T, x1)\n",
    "    x3 = x3 + x2\n",
    "  \n",
    "  a = (1 / len(X)) * x3\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641848978098,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "dTtMltLiJsA2"
   },
   "outputs": [],
   "source": [
    "def calculate_log_likelihood_2b(X, theta, a):\n",
    "  \"\"\"\n",
    "   Calculates log likelihood by the equation described in the next section.\n",
    "  \"\"\"\n",
    "  x1 = (-(len(X) * len(X[0])) / 2) * np.log(2 * math.pi)\n",
    "  x2 = (-len(X) / 2) * np.log(np.linalg.det(a))\n",
    "  x3 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x4 = X[i] - theta.T\n",
    "    x5 = np.matmul(x4, np.linalg.inv(a))\n",
    "    x6 = np.matmul(x5, x4.T)\n",
    "    x3 = x3 + x6\n",
    "\n",
    "  x3 = (-1/2) * x3\n",
    "\n",
    "  L = x1 + x2 + x3\n",
    "\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBmuX8rkRi3A"
   },
   "source": [
    "## Problem 2 - Question 2b\n",
    "\n",
    "Again, what we want to do is obtain an estimate for the probability density functions (pdfs) of our 2 classes. We have our training set, and now we are making the assumption that the distribution of our pdf's is guassian, but with  covariance matrices that are not diagonal. In order to find our estimates, we are going to implement the Maximum Likelihood method (ML) once again, so we are going to according to maximize the log likelihood with respect to $\\vec{\\theta}$ and $Σ$: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Log likelihood is equal to: $\\Pi(\\vec{\\theta},\\Sigma)=ln(\\prod_{k=1}^N p(\\vec{x_{k}}|\\vec{\\theta}))=\n",
    "-Nln((2\\pi)^{\\frac{n}{2}})-Nln(|\\Sigma|^{\\frac{1}{2}})-\\frac{1}{2}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})^T\\Sigma^{-1}(\\vec{x_k}-\\vec{\\theta})$.\n",
    "\n",
    "\n",
    "\n",
    "Using the property $trace(ba^T)=a^Tb$ with $a, b$ real valued matrices we obtain: $x^TAx=x^T(Ax)=trace(A^Tx^Tx)=trace(Axx^T)$ with $A$ a symmetric matrix. \n",
    "\n",
    "\n",
    "The Log likelihood becomes:\n",
    "\n",
    " $\\Pi(\\vec{\\theta},\\Sigma)=\n",
    "-Nln((2\\pi)^{\\frac{n}{2}})+\\frac{1}{2}Nln(|\\Sigma|^{-1})-\\frac{1}{2}\\sum_{k=1}^Ntrace[(\\vec{x_k}-\\vec{\\theta})(\\vec{x_k}-\\vec{\\theta})^T\\Sigma^{-1}]\n",
    "=\n",
    "-Nln((2\\pi)^{\\frac{n}{2}})+\\frac{1}{2}Nln(|\\Sigma|^{-1})-\\frac{1}{2}trace[\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})(\\vec{x_k}-\\vec{\\theta})^T\\Sigma^{-1}]$\n",
    "\n",
    "\n",
    "By taking the same partial derivatives equal to 0 as before:\n",
    "\n",
    "\n",
    "* $\\frac{\\partial \\Pi}{\\partial \\Sigma^{-1}} = 0 \\rightarrow \\frac{N}{2}((\\Sigma^{-1})^{-1})^T-\\frac{1}{2}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})(\\vec{x_k}-\\vec{\\theta})^T = 0 \\rightarrow\n",
    "\\Sigma = \\frac{1}{N}\\sum_{k=1}^N(\\vec{x_k}-\\vec{\\theta})(\\vec{x_k}-\\vec{\\theta})^T$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial \\Pi}{\\partial \\vec{\\theta}}=0 \\rightarrow ... \\rightarrow \\vec{\\theta_{ML}}=\\frac{1}{N}\\sum_{k=1}^N\\vec{x_k}$\n",
    "\n",
    "The equations were adapted from: https://groups.seas.harvard.edu/courses/cs281/files/section01.pdf?fbclid=IwAR2Irz2_entsrh5nGDYAV9F2EkoZUVQdm3by-Y01oKp5yR3svs7kLtRxI4U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1641849052641,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "1i4rhCfDTR5L",
    "outputId": "4addab23-a7bf-4381-bf00-c8605f03d7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of mean for class 0:\n",
      " [[  3.298   ]\n",
      " [109.98    ]\n",
      " [ 68.184   ]\n",
      " [ 19.664   ]\n",
      " [ 68.792   ]\n",
      " [ 30.3042  ]\n",
      " [  0.429734]\n",
      " [ 31.19    ]]\n",
      "Estimation of covariance matrix for class 0:\n",
      " [[ 9.08519600e+00  7.76796000e+00  7.23916800e+00 -5.30587200e+00\n",
      "  -3.92920160e+01  3.81948400e-01 -7.20027320e-02  2.01233800e+01]\n",
      " [ 7.76796000e+00  6.81995600e+02  9.08536800e+01  6.22128000e+00\n",
      "   9.10377840e+02  2.64314840e+01  7.45542680e-01  6.94078000e+01]\n",
      " [ 7.23916800e+00  9.08536800e+01  3.25622144e+02  5.02138240e+01\n",
      "   1.33002272e+02  5.03454272e+01  1.47144944e-01  4.51570400e+01]\n",
      " [-5.30587200e+00  6.22128000e+00  5.02138240e+01  2.21267104e+02\n",
      "   6.06452112e+02  5.01206112e+01  4.23028624e-01 -2.83981600e+01]\n",
      " [-3.92920160e+01  9.10377840e+02  1.33002272e+02  6.06452112e+02\n",
      "   9.75479674e+03  1.92872674e+02  6.71014467e+00 -1.71800480e+02]\n",
      " [ 3.81948400e-01  2.64314840e+01  5.03454272e+01  5.01206112e+01\n",
      "   1.92872674e+02  5.90156024e+01  1.62197517e-01  3.22980200e+00]\n",
      " [-7.20027320e-02  7.45542680e-01  1.47144944e-01  4.23028624e-01\n",
      "   6.71014467e+00  1.62197517e-01  8.92731152e-02  1.45104540e-01]\n",
      " [ 2.01233800e+01  6.94078000e+01  4.51570400e+01 -2.83981600e+01\n",
      "  -1.71800480e+02  3.22980200e+00  1.45104540e-01  1.35861900e+02]]\n"
     ]
    }
   ],
   "source": [
    "# Estimates for class 0\n",
    "theta_ml_0 = calculate_theta_2b(X_0)\n",
    "a_0 = calculate_covariance_2b(X_0, theta_ml_0)\n",
    "\n",
    "print(f\"Estimation of mean for class 0:\\n {theta_ml_0}\")\n",
    "print(f\"Estimation of covariance matrix for class 0:\\n {a_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641849056090,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "hFrnlisWMUTU",
    "outputId": "5ddd05bb-0483-4c4c-908a-f342f6db8538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of mean for class 1:\n",
      " [[  4.86567164]\n",
      " [141.25746269]\n",
      " [ 70.82462687]\n",
      " [ 22.1641791 ]\n",
      " [100.3358209 ]\n",
      " [ 35.14253731]\n",
      " [  0.5505    ]\n",
      " [ 37.06716418]]\n",
      "Estimation of covariance matrix for class 1:\n",
      " [[ 1.39446425e+01 -6.49899755e+00  1.01704723e+01 -5.21675206e+00\n",
      "  -4.06116061e+01 -4.30622633e+00 -9.60335821e-02  1.81918579e+01]\n",
      " [-6.49899755e+00  1.01633297e+03  4.69817192e+01  2.11629539e+01\n",
      "   1.15345458e+03  1.16521079e+01  3.13677239e-01  3.44006182e+01]\n",
      " [ 1.01704723e+01  4.69817192e+01  4.60174468e+02  8.52675986e+01\n",
      "   2.65379789e+02  2.08309674e+01  2.75236940e-01  6.16908833e+01]\n",
      " [-5.21675206e+00  2.11629539e+01  8.52675986e+01  3.11405881e+02\n",
      "   1.11529561e+03  3.99210013e+01  1.79638806e+00 -1.77759523e+01]\n",
      " [-4.06116061e+01  1.15345458e+03  2.65379789e+02  1.11529561e+03\n",
      "   1.91629021e+04  5.53069837e+01  5.22539925e+00  3.62871464e+01]\n",
      " [-4.30622633e+00  1.16521079e+01  2.08309674e+01  3.99210013e+01\n",
      "   5.53069837e+01  5.25538622e+01  3.68475000e-01 -1.49215137e+01]\n",
      " [-9.60335821e-02  3.13677239e-01  2.75236940e-01  1.79638806e+00\n",
      "   5.22539925e+00  3.68475000e-01  1.38130519e-01 -3.58541045e-01]\n",
      " [ 1.81918579e+01  3.44006182e+01  6.16908833e+01 -1.77759523e+01\n",
      "   3.62871464e+01 -1.49215137e+01 -3.58541045e-01  1.19853698e+02]]\n"
     ]
    }
   ],
   "source": [
    "# Estimates for class 1\n",
    "theta_ml_1= calculate_theta_2b(X_1)\n",
    "a_1 = calculate_covariance_2b(X_1, theta_ml_1)\n",
    "\n",
    "print(f\"Estimation of mean for class 1:\\n {theta_ml_1}\")\n",
    "print(f\"Estimation of covariance matrix for class 1:\\n {a_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1641849060451,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "Xp-4ds8ZMXk8"
   },
   "outputs": [],
   "source": [
    "# Finding the log likelihoods of each class and calculating AIC and BIC criterion.\n",
    "k = 2\n",
    "\n",
    "log_likelihood_1 = calculate_log_likelihood_2b(X_1, theta_ml_1, a_1)\n",
    "\n",
    "log_likelihood_0 = calculate_log_likelihood_2b(X_0, theta_ml_0, a_0)\n",
    "\n",
    "aic_1 = calculate_AIC(k, log_likelihood_1)\n",
    "bic_1 = calculate_BIC(k, log_likelihood_1, len(X_1))\n",
    "\n",
    "aic_0 = calculate_AIC(k, log_likelihood_0)\n",
    "bic_0 = calculate_BIC(k, log_likelihood_0, len(X_0))\n",
    "\n",
    "\n",
    "AIC_0.append(aic_0)\n",
    "AIC_1.append(aic_1)\n",
    "BIC_0.append(bic_0)\n",
    "BIC_1.append(bic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8IhWod-0DKQ"
   },
   "source": [
    "## Functions used in Problem 3 - Question 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1641849064437,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "vYmJfILwMeRF"
   },
   "outputs": [],
   "source": [
    "def bayes_classifier_3b(x, th_1, th_0, a_1, a_0):\n",
    "  \"\"\"\n",
    "    Classifies points x to classes: 1 (with its distribution having mean th_1 and cov.matrix a_1) and 0 (with its distribution having mean th_0 and cov.matrix a_0).\n",
    "    The classification is done by calculating the Mahalanobis distance [(x-mean_value)^T cov.matrix^{-1} (x-mean_value)] between the point x and the classes.\n",
    "  \"\"\"\n",
    "  x = np.reshape(x,(1,8))\n",
    "  x1 = np.matmul((x - th_1.T), np.linalg.inv(a_1))\n",
    "  a = np.matmul(x1, (x - th_1.T).T)\n",
    "\n",
    "  x3 = np.matmul((x - th_0.T), np.linalg.inv(a_0))\n",
    "  b = np.matmul(x3, (x - th_0.T).T)\n",
    "  \n",
    "  if a > b:\n",
    "    # Classify point to class 0.\n",
    "    return 0\n",
    "  else:\n",
    "    # Classify point to class 1.\n",
    "    return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1641849068242,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "-CWD577xECYH"
   },
   "outputs": [],
   "source": [
    "def produce_y_pred_3b(X_tst, th_1, th_0, a_1, a_0):\n",
    "    \"\"\"\n",
    "        Performs classification using the Bayes classifier to the test set and collects the\n",
    "        results to a vector.\n",
    "    \"\"\"\n",
    "    Y_pred = []\n",
    "\n",
    "    for i in range(len(X_tst)):\n",
    "      Y_pred.append(bayes_classifier_3b(X_tst[i], th_1, th_0, a_1, a_0))\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1641849070332,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "FaqFRs51EpbW"
   },
   "outputs": [],
   "source": [
    "def cross_validation_3b(k):\n",
    "  \"\"\"\n",
    "    Performs k-fold validation in the dataset, finding the average error between all folds in order to give a not biased view \n",
    "    of the performance of the classifier.\n",
    "  \"\"\"\n",
    "  accu = []\n",
    "\n",
    "  kf = KFold(n_splits=k, random_state=None)\n",
    "  \n",
    "  for i , j in kf.split(np_X):\n",
    "    X_train , X_test = np_X[i,:], np_X[j,:]\n",
    "    y_train , y_test = Y[i], Y[j]\n",
    "\n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "\n",
    "    for i, value in enumerate(X_train):\n",
    "      if y_train[i] == 1:\n",
    "        X_1.append(value)\n",
    "      else:\n",
    "        X_0.append(value)\n",
    "\n",
    "    theta_ml_1= calculate_theta_2b(X_1)\n",
    "    a_1 = calculate_covariance_2b(X_1, theta_ml_1)\n",
    "\n",
    "\n",
    "    theta_ml_0= calculate_theta_2b(X_0)\n",
    "    a_0 = calculate_covariance_2b(X_0, theta_ml_0)\n",
    "\n",
    "\n",
    "    y_pred = produce_y_pred_3b(X_test, theta_ml_1, theta_ml_0, a_1, a_0)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accu.append(acc)\n",
    "     \n",
    "  avg_accu = sum(accu)/k\n",
    "  \n",
    "  return avg_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9__miMCVNfd"
   },
   "source": [
    "## Problem 3 - Question 3b\n",
    "\n",
    "In this question, we are dealing with the case where the covariance matrix is not diagonal ($\\Sigma \\neq \\sigma^2 I$). According to calculations we performed in the lectures, we concluded that we can perform classification using the Mahalanobis distance: \n",
    "\n",
    "We assign $\\vec{x}$ to $\\omega_1$ if $(\\vec{x}-\\vec{\\mu_1})^T\\Sigma_1^{-1}(\\vec{x}-\\vec{\\mu_1}) < (\\vec{x}-\\vec{\\mu_2})^T\\Sigma_2^{-1}(\\vec{x}-\\vec{\\mu_2})$ (with the equivalent relationship for class $\\omega_2$). \n",
    "\n",
    "So, the Bayes classifier we designed calculates the Mahalanobis distances between the query vector we want to classify, and the estimations of the means of the pdfs that correspond to each one of our classes, which we have calculated using the ML method.\n",
    "\n",
    "In order to calculate the accuracy of our classifier, we are going to implement K-fold cross validation for K = 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1641849074665,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "zAgzW5LX14br"
   },
   "outputs": [],
   "source": [
    "# Obtain average accuracy from Kfold (for K = 10)\n",
    "avg_acc_3b = cross_validation_3b(10)\n",
    "\n",
    "average_accuracy.append(avg_acc_3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPi3pOmfY389"
   },
   "source": [
    "## Functions used in Problem 2 - Questions 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641849075898,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "qHHIigImuW0b"
   },
   "outputs": [],
   "source": [
    "def calculate_theta_2c(X):\n",
    "  \"\"\"\n",
    "   Calculates vector thetas containing the mean value theta_j of each one of the l features of the dataset.\n",
    "  \"\"\"\n",
    "  thetas = []\n",
    "\n",
    "  for j in range(len(X[0])):\n",
    "    x1 = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "      x1 = x1 + X[i][j]\n",
    "  \n",
    "    thetas.append((1/len(X)) * x1)\n",
    "\n",
    "  thetas = np.array(thetas, dtype=np.float64)\n",
    "  thetas = np.reshape(thetas,(len(thetas),1))\n",
    "  return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1641849078710,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "oJq8PeEkvCRk"
   },
   "outputs": [],
   "source": [
    "def calculate_s_2c(X, theta):\n",
    "  \"\"\"\n",
    "   Calculates covariances sigma_j^2 for the Gaussian distributions of each feature.\n",
    "  \"\"\"\n",
    "  s = []\n",
    "  x2 = 0\n",
    "\n",
    "  for j in range(len(X[0])):\n",
    "    x1 = 0\n",
    "    x2 = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "      x1 = X[i][j] - theta[j]\n",
    "      x2 = x2 + np.power(x1, 2)\n",
    "  \n",
    "    s.append((1 / len(X)) * x2)\n",
    "  \n",
    "  s = np.array(s, dtype=np.float64)\n",
    "  s = np.reshape(s,(len(s),1))\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1641849080802,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "2Qp-qUUzyAY-"
   },
   "outputs": [],
   "source": [
    "def calculate_log_likelihood_2c(X, theta, s):\n",
    "  \"\"\"\n",
    "    Calculates log likelihood with the formulas derived in the next section.\n",
    "  \"\"\"\n",
    "  L = 0 \n",
    "\n",
    "  for i in range(len(X)):\n",
    "\n",
    "    x1 = calculate_P_ol_2c(X[i], theta, s)\n",
    "    x2 = math.log(x1)\n",
    "    L = L + x2\n",
    "\n",
    "  return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1641849082999,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "MXi3KL_86lbT"
   },
   "outputs": [],
   "source": [
    "def calculate_P_ol_2c(X, theta, s):\n",
    "  \"\"\"\n",
    "    Calculates probability density function at a point X, given vectors containing the mean values and the covariances\n",
    "    of each Gaussian distribution.\n",
    "  \"\"\"\n",
    "  x1 = 1\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x2 = (1 / math.sqrt(2 * math.pi)) * (1 / s[i])\n",
    "    x3 = -(np.power((X[i] - theta[i]), 2) / (2 * np.power(s[i], 2)))\n",
    "    x4 = np.power(math.e, x3)\n",
    "    \n",
    "    x1 = x1 * x2 * x4\n",
    "\n",
    "  return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBLKOkjBZSFw"
   },
   "source": [
    "## Problem 2 - Question 2c\n",
    "\n",
    "Now we are making the assumption that the components of the vectors in our data set are mutually statistically independent. This allows us to do all calculations in seperate dimensions:\n",
    "\n",
    "$p(\\vec{x}|\\vec{\\theta}) = p(x_{1}|θ_{1})* ....* p(x_{\\lambda}|θ_{\\lambda})$\n",
    "\n",
    "where: $\\vec{x} = [x_{1},...,x_{\\lambda}], \\vec{θ} = [\\theta_{1},...,\\theta_{\\lambda}]$, $\\lambda$: number of features.\n",
    "\n",
    "Also, we are assuming that the marginal pdf's are guassian:\n",
    "\n",
    "For each feature $i$: $p(x_{i}|θ_{i})=\\frac{1}{(2\\pi)^{\\frac{1}{2}} σ_{i}}exp(-\\frac{(x_{i} - θ_{i})^2}{2σ_{i}^{2}})$\n",
    "\n",
    "We are going to implement the ML method in order to find estimates for their means and variances. So we are implementing it in 1D, using each time only the components of a particular dimension from the vectors in our data set.\n",
    "According to the ML method:\n",
    "\n",
    "Log likelihood: $\\Pi({\\vec{\\theta}},\\vec{σ})=ln(\\prod_{k=1}^N p(x_k|\\theta))=ln(\\prod_{k=1}^N (\\prod_{i=1}^l \\frac{1}{(2\\pi)^{\\frac{1}{2}} σ_{i}}exp(-\\frac{(x_{ki} - θ_{i})^2}{2σ_{i}^{2}})))\n",
    "=ln(\\prod_{k=1}^N    [\\frac{1}{(2\\pi)^{\\frac{1}{2}}}]^\\lambda \\frac{1}{\\sigma_1 \\cdot .. \\sigma_\\lambda} exp     (\\sum_{i=1}^\\lambda -\\frac{(x_{ki} - θ_{i})^2}{2σ_{i}^{2}}))=\n",
    "-Nln((2\\pi)^{\\frac{\\lambda}{2}})-Nln(\\prod_{i=1}^\\lambda \\sigma_i)-\\sum_{k=1}^N \\sum_{i=1}^\\lambda \\frac{(x_{ki}-\\theta_i)^2}{2\\sigma_i^2}$\n",
    "\n",
    "\n",
    "We then take the partial derivatives with respect to $θ_{j}$ and $σ_{j}$ for every $j\\epsilon[1,l]$:\n",
    "* $\\frac{\\partial \\Pi}{\\partial σ_{j}}=0 \n",
    "\\rightarrow 0 - N\\frac{1}{σ_{j}}-\\frac{1}{2}(-2)\\sum_{k=1}^N\\frac{(x_{kj}-\\theta_{j})^2}{σ_{j}^{3}} = 0 \\rightarrow  \\sum_{k=1}^N\\frac{(x_{kj}-\\theta_{j})^2}{σ_{j}^{3}} =  N\\frac{1}{σ_{j}} \\rightarrow σ_{j}^{2} =  \\sum_{k=1}^N\\frac{(x_{kj}-\\theta_{j})^2}{N}$\n",
    "\n",
    "* $\\frac{\\partial \\Pi}{\\partial \\theta_j} = 0 \\rightarrow \\frac{1}{σ_j^{2}}\n",
    "\\sum_{k=1}^N(x_{kj}-{\\theta_{j}}){}=0 \\rightarrow \\sum_{k=1}^N(x_{nj})= N\\theta_j \\rightarrow \\theta_j = \\frac{\\sum_{k=1}^N(x_{nj})}{N} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1641849090348,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "lGqdn7MDzghl",
    "outputId": "633b6b55-afb0-45f6-e4df-359839be0134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.86567164]\n",
      " [141.25746269]\n",
      " [ 70.82462687]\n",
      " [ 22.1641791 ]\n",
      " [100.3358209 ]\n",
      " [ 35.14253731]\n",
      " [  0.5505    ]\n",
      " [ 37.06716418]]\n",
      "[[1.39446425e+01]\n",
      " [1.01633297e+03]\n",
      " [4.60174468e+02]\n",
      " [3.11405881e+02]\n",
      " [1.91629021e+04]\n",
      " [5.25538622e+01]\n",
      " [1.38130519e-01]\n",
      " [1.19853698e+02]]\n"
     ]
    }
   ],
   "source": [
    "# Estimates for class 1\n",
    "theta_ml_1= calculate_theta_2c(X_1)\n",
    "s_1 = calculate_s_2c(X_1, theta_ml_1)\n",
    "\n",
    "print(theta_ml_1)\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1641849094539,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "IRzAAoaCzg_d",
    "outputId": "2768fca4-0c96-49e7-a527-537df7634d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.298   ]\n",
      " [109.98    ]\n",
      " [ 68.184   ]\n",
      " [ 19.664   ]\n",
      " [ 68.792   ]\n",
      " [ 30.3042  ]\n",
      " [  0.429734]\n",
      " [ 31.19    ]]\n",
      "[[9.08519600e+00]\n",
      " [6.81995600e+02]\n",
      " [3.25622144e+02]\n",
      " [2.21267104e+02]\n",
      " [9.75479674e+03]\n",
      " [5.90156024e+01]\n",
      " [8.92731152e-02]\n",
      " [1.35861900e+02]]\n"
     ]
    }
   ],
   "source": [
    "# Estimates for class 0\n",
    "theta_ml_0 = calculate_theta_2c(X_0)\n",
    "s_0 = calculate_s_2c(X_0, theta_ml_0)\n",
    "\n",
    "print(theta_ml_0)\n",
    "print(s_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1641849098172,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "Abi8ZDVM0Jqp"
   },
   "outputs": [],
   "source": [
    "# Finding the log likelihoods of each class and calculating AIC and BIC criterion.\n",
    "k = 2\n",
    "log_likelihood_1 = calculate_log_likelihood_2c(X_1, theta_ml_1, s_1)\n",
    "\n",
    "log_likelihood_0 = calculate_log_likelihood_2c(X_0, theta_ml_0, s_0)\n",
    "\n",
    "aic_1 = calculate_AIC(k, log_likelihood_1)\n",
    "bic_1 = calculate_BIC(k, log_likelihood_1, len(X_1))\n",
    "\n",
    "aic_0 = calculate_AIC(k, log_likelihood_0)\n",
    "bic_0 = calculate_BIC(k, log_likelihood_0, len(X_0))  \n",
    "\n",
    "AIC_0.append(aic_0)\n",
    "AIC_1.append(aic_1)\n",
    "BIC_0.append(bic_0)\n",
    "BIC_1.append(bic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1E4O9Ro6CmW"
   },
   "source": [
    "## Functions used in Problem 3 - Question 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1641849100486,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "X6RkW0RXjzHM"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_classifier_3c(x, th_1, th_0, a_1, a_0):\n",
    "  \"\"\"\n",
    "    Naive Bayes Classifier, classifies to w_1 if p(x|w_1)>p(x|w_2)\n",
    "  \"\"\"\n",
    "  a = calculate_P_ol_2c(x , th_1, a_1)\n",
    "  b = calculate_P_ol_2c(x, th_0, a_0)\n",
    "\n",
    "  if a > b:\n",
    "    # Classify to class 1.\n",
    "    return 1\n",
    "  else:\n",
    "    # Classify to class 0.\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1641849102970,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "s24udpeqAZrM"
   },
   "outputs": [],
   "source": [
    "def produce_y_pred_3c(X_tst, th_1, th_0, a_1, a_0):\n",
    "    \"\"\"\n",
    "        Performs classification using the Bayes classifier to the test set and collects the\n",
    "        results to a vector.\n",
    "    \"\"\"\n",
    "    Y_pred = []\n",
    "\n",
    "    for i in range(len(X_tst)):\n",
    "      Y_pred.append(naive_bayes_classifier_3c(X_tst[i], th_1, th_0, a_1, a_0))\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641849104110,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "hDPeX_WQAjIr"
   },
   "outputs": [],
   "source": [
    "def cross_validation_3c(k):\n",
    "  \"\"\"\n",
    "    Performs k-fold validation in the dataset, finding the average error between all folds in order to give a not biased view \n",
    "    of the performance of the classifier.\n",
    "  \"\"\"\n",
    "  accu = []\n",
    "\n",
    "  kf = KFold(n_splits=k, random_state=None)\n",
    "  \n",
    "  for i , j in kf.split(np_X):\n",
    "    X_train , X_test = np_X[i,:], np_X[j,:]\n",
    "    y_train , y_test = Y[i], Y[j]\n",
    "\n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "\n",
    "    for i, value in enumerate(X_train):\n",
    "      if y_train[i] == 1:\n",
    "        X_1.append(value)\n",
    "      else:\n",
    "        X_0.append(value)\n",
    "\n",
    "    theta_ml_1= calculate_theta_2c(X_1)\n",
    "    s_1 = calculate_s_2c(X_1, theta_ml_1)\n",
    "\n",
    "    theta_ml_0= calculate_theta_2c(X_0)\n",
    "    s_0 = calculate_s_2c(X_0, theta_ml_0)\n",
    "\n",
    "    y_pred = produce_y_pred_3c(X_test, theta_ml_1, theta_ml_0, s_1, s_0)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accu.append(acc)\n",
    "     \n",
    "  avg_accu = sum(accu)/k\n",
    "  \n",
    "  return avg_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ_Xdmx46fFe"
   },
   "source": [
    "## Problem 3 - Question 3c\n",
    "\n",
    "In order to decide which class we are going to assign to each of the samples of our test set, we are going to implement the rule we mentioned before (which holds assuming that P($ω_{1}$) = P($ω_{2}$)): \n",
    "\n",
    "assign x⃗  to ω1 if: $p(\\vec{x}|\\omega_1) > p(\\vec{x}|\\omega_2)$ and to ω2 if $p(\\vec{x}|\\omega_1) < p(\\vec{x}|\\omega_2)$.\n",
    "\n",
    "We have made the assumption that the components of the feature vectors are mutually statistically independent, and so each one of our pdf's are the products of marginal 1d pdf's, which are gaussian and whose parameters we calculate using our training set and implementing the ML method. In this case, we say that we are implementing a Naive Bayes classifier.\n",
    "\n",
    "In order to calculate the accuracy of our classifier, once again we are going to implement K-fold cross validation for K = 10. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1641849107907,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "xdHB9meYDGZ8"
   },
   "outputs": [],
   "source": [
    "# Obtain average accuracy from Kfold (for K = 10)\n",
    "avg_acc_3c = cross_validation_3c(10)\n",
    "\n",
    "average_accuracy.append(avg_acc_3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3ae4ZQdeWOw"
   },
   "source": [
    "## Functions used in Problem 2 - Questions 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641849110507,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "HCd0bbPoesVQ"
   },
   "outputs": [],
   "source": [
    "def kernel_2d(x):\n",
    "  \"\"\"\n",
    "    Kernel of a 2-D Gaussian distribution N(0,1) : mean value 0 and variance 1\n",
    "  \"\"\"\n",
    "  x1 = (1 / math.sqrt(2 * math.pi))\n",
    "  x2 = -(np.power(x, 2) / 2)\n",
    "  x3 = np.power(math.e, x2)\n",
    "    \n",
    "  x4 = x1 * x3\n",
    "\n",
    "  return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1641849112657,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "p3JilH45ikzr"
   },
   "outputs": [],
   "source": [
    "def calculate_p_2d(x, X, l, h):\n",
    "  \"\"\"\n",
    "    Calculates probability distribution for one feature using the equations derived in the following section. \n",
    "  \"\"\"\n",
    "  x1 = 1 / (len(X) * h)\n",
    "  x2 = 0\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    x3 =  (X[i][l] - x) / h \n",
    "    y = kernel_2d(x3)\n",
    "\n",
    "    x2 = x2 + y\n",
    "  \n",
    "  p = x1 * x2\n",
    "\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1641849115507,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "eNleCeNbiLL6"
   },
   "outputs": [],
   "source": [
    "def calculate_P_ol_2d(x, X, h):\n",
    "  \"\"\"\n",
    "    Calculates probability distribution for all features using the equations derived in the following section. \n",
    "  \"\"\"\n",
    "  x1 = 1\n",
    "\n",
    "  for i in range(len(x)):\n",
    "    x2 = calculate_p_2d(x[i], X, i, h)\n",
    "    \n",
    "    x1 = x1 * x2\n",
    "\n",
    "  return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ2z971QKaJ5"
   },
   "source": [
    "## Problem 2 - Question 2d\n",
    "\n",
    "For once again we assume that the components of the feature vectors are mutually statistically independent:\n",
    "\n",
    "$p(\\vec{x}) = p(x_{1})* ....* p(x_{\\lambda})$,\n",
    "\n",
    "where: $\\vec{x} = [x_{1},...,x_{λ}]$, λ: number of features.\n",
    "\n",
    "\n",
    "Now, we are going to use only our data set, in order to directly estimate a function that we think will be a good aproximation to the above pdf. \n",
    "We will do this for each one of the marginal pdfs, using a method called 1d Parzen windows.\n",
    "We will implement it with gaussian kernels, and we will take the width h of each window to be equal to the square root of the number of patterns in our data set. So:\n",
    "\n",
    "$p(x_{l})$ = $\\frac{1}{hN}$$\\sum_{k=1}^Ng(\\frac{x_{kl}-x_{l}}{h})$, $l\\in[1,λ]$\n",
    "\n",
    "We assume the mean and variance of our gaussian kernel to be 0 and 1 respectively, and so:\n",
    "\n",
    "$g(x) = \\frac{1}{(2\\pi)^{\\frac{1}{2}}}exp(-\\frac{(x)^2}{2})$ $\\rightarrow$ g($\\frac{x_{kl}-x_{l}}{h}$) = $\\frac{1}{(2\\pi)^{\\frac{1}{2}}}exp(-\\frac{(x_{kl}-x_{l})^2}{2h^{2}})$\n",
    "\n",
    "This results in:\n",
    "\n",
    "$p(x_{l})$ = $\\frac{1}{(2\\pi)^{\\frac{1}{2}}Νh}\\sum_{k=1}^Nexp(-\\frac{(x_{kl}-x_{l})^2}{2h^{2}}) = \\frac{1}{Ν}\\sum_{k=1}^Ng(x_{k})$\n",
    "\n",
    "being the mean of N 1-d Gaussians of mean $x_{l}$ and variance $h^{2} = N$.\n",
    "\n",
    "So, in overall, we get:\n",
    "\n",
    "$p(\\vec{x})$ = $\\prod_{l=1}^{λ}\\sum_{k=1}^N\\frac{g(x_{n})}{N}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nVOq3aFetQP"
   },
   "source": [
    "##Comments\n",
    "The parzen windows method is a non parametric method. We get an estimation for our pdf, not for parameters. So we didnt use the AIC and BIC criterions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zvWHjbgFyjf"
   },
   "source": [
    "## Functions used in Problem 3 - Question 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1641849119987,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "HkmYn94yj4hx"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_classifier_3d(x, X_1, X_0):\n",
    "  \"\"\"\n",
    "    Naive Bayes Classifier, classifies to w_1 if p(x|w_1)>p(x|w_2)\n",
    "  \"\"\"\n",
    "  a = calculate_P_ol_2d(x, X_1, np.sqrt(len(X_1)))\n",
    "\n",
    "  b = calculate_P_ol_2d(x, X_0, np.sqrt(len(X_0)))\n",
    "\n",
    "  if a > b:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641849121371,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "SKfeTVa4oo-x"
   },
   "outputs": [],
   "source": [
    "def produce_y_pred_3d(X_tst, X_1, X_0):\n",
    "    \"\"\"\n",
    "      Performs classification using the Bayes classifier to the test set and collects the\n",
    "      results to a vector.\n",
    "    \"\"\"\n",
    "    Y_pred = []\n",
    "\n",
    "    for i in range(len(X_tst)):\n",
    "      Y_pred.append(naive_bayes_classifier_3d(X_tst[i], X_1, X_0))\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1641849123984,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "Al9nHPAVoq27"
   },
   "outputs": [],
   "source": [
    "def cross_validation_3d(k):\n",
    "  \"\"\"\n",
    "    Performs k-fold validation in the dataset, finding the average error between all folds in order to give a not biased view \n",
    "    of the performance of the classifier.\n",
    "  \"\"\"\n",
    "  accu = []\n",
    "\n",
    "  kf = KFold(n_splits=k, random_state=None)\n",
    "  \n",
    "  for i , j in kf.split(np_X):\n",
    "    X_train , X_test = np_X[i,:], np_X[j,:]\n",
    "    y_train , y_test = Y[i], Y[j]\n",
    "\n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "\n",
    "    for i, value in enumerate(X_train):\n",
    "      if y_train[i] == 1:\n",
    "        X_1.append(value)\n",
    "      else:\n",
    "        X_0.append(value)\n",
    "\n",
    "    y_pred = produce_y_pred_3d(X_test, X_1, X_0)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accu.append(acc)\n",
    "     \n",
    "  avg_accu = sum(accu)/k\n",
    "  \n",
    "  return avg_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEJGFoUeF5yv"
   },
   "source": [
    "## Problem 3 - Question 3d\n",
    "\n",
    "In order to decide which class we are going to assign to each of the samples of our test set, we are going to implement the rule we mentioned before (which holds assuming that $P(ω_{1}) = P(ω_{2}))$: \n",
    "\n",
    "Assign $\\vec{x}$  to $ω_1$ if: $p(\\vec{x}|\\omega_1) > p(\\vec{x}|\\omega_2)$ and to $ω_2$ if $p(\\vec{x}|\\omega_1) < p(\\vec{x}|\\omega_2)$.\n",
    "\n",
    "We have made the assumption that the components of our feature vectors are mutually statistically independent, and so we do all calculations in seperate dimensions. So, again we are implementing a Naive Bayes classifier.\n",
    "\n",
    "We split our data set, in training and test set, and we use the first in order to estimate a pdf for each one of our classes, using a method called 1d Parzen windows. Then we use our model to perform classification on the test set.\n",
    "\n",
    "In order to calculate the accuracy of our classifier, we are going to implement K-fold cross validation for K = 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 28972,
     "status": "ok",
     "timestamp": 1641849155916,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "9vywCOoOF5b6"
   },
   "outputs": [],
   "source": [
    "# Obtain average accuracy from Kfold (for K = 10)\n",
    "avg_acc_3d = cross_validation_3d(10)\n",
    "\n",
    "average_accuracy.append(avg_acc_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4-WqXdwkDxd"
   },
   "source": [
    "## Results and Comments for Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1641849158341,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "vT5q7pOTQsC_"
   },
   "outputs": [],
   "source": [
    "# Helper function for cleaning the results\n",
    "def cleanAIC_BIC(criterion):\n",
    "  x = [criterion[0][0][0],\n",
    "       criterion[1][0][0],\n",
    "       criterion[2]]\n",
    "  return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1641849159758,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "iXXUVu_sl2PZ",
    "outputId": "c46a82fb-3880-4973-b88e-1a3c60de6812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC criterion for class 0:\n",
      "[25228.06602106212, 28492.823315258855, 48703.74336561089]\n",
      "AIC criterion for class 1:\n",
      "[14206.128906907672, 16027.863890855155, 26313.595061925662]\n",
      "BIC criterion for class 0:\n",
      "[25236.495237258965, 28501.2525314557, 48712.17258180774]\n",
      "BIC criterion for class 1:\n",
      "[14213.310880868694, 16035.045864816177, 26320.777035886684]\n",
      "\n",
      "\n",
      "Deltas AIC of models b and c from a for class 0:\n",
      "[3264.7572941967337, 23475.67734454877]\n",
      "Deltas AIC of models b and c from a for class 1:\n",
      "[1821.7349839474828, 12107.46615501799]\n",
      "\n",
      "\n",
      "Deltas BIC of models b and c from a for class 0:\n",
      "[3264.7572941967337, 23475.677344548774]\n",
      "Deltas BIC of models b and c from a for class 1:\n",
      "[1821.7349839474828, 12107.46615501799]\n"
     ]
    }
   ],
   "source": [
    "AIC_0_cl = cleanAIC_BIC(AIC_0)\n",
    "AIC_1_cl = cleanAIC_BIC(AIC_1)\n",
    "BIC_0_cl = cleanAIC_BIC(BIC_0)\n",
    "BIC_1_cl = cleanAIC_BIC(BIC_1)\n",
    "\n",
    "print(f\"AIC criterion for class 0:\\n{AIC_0_cl}\")\n",
    "print(f\"AIC criterion for class 1:\\n{AIC_1_cl}\")\n",
    "print(f\"BIC criterion for class 0:\\n{BIC_0_cl}\")\n",
    "print(f\"BIC criterion for class 1:\\n{BIC_1_cl}\")\n",
    "\n",
    "print(f\"\\n\\nDeltas AIC of models b and c from a for class 0:\\n{[AIC_0_cl[1]-AIC_0_cl[0], AIC_0_cl[2]-AIC_0_cl[0]]}\")\n",
    "print(f\"Deltas AIC of models b and c from a for class 1:\\n{[AIC_1_cl[1]-AIC_1_cl[0], AIC_1_cl[2]-AIC_1_cl[0]]}\")\n",
    "\n",
    "print(f\"\\n\\nDeltas BIC of models b and c from a for class 0:\\n{[BIC_0_cl[1]-BIC_0_cl[0], BIC_0_cl[2]-BIC_0_cl[0]]}\")\n",
    "print(f\"Deltas BIC of models b and c from a for class 1:\\n{[BIC_1_cl[1]-BIC_1_cl[0], BIC_1_cl[2]-BIC_1_cl[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRWqBCRDQTMO"
   },
   "source": [
    "Examining the AIC Criterion, we can see that the (a) model outperforms the other 2 in both classes. If we calculate the delta AICs of the other two models from the optimal model (a), they end up extremely greater than 10, so they are surely not candidates for the best model to fit the distribution of the specific dataset.\n",
    "\n",
    "\n",
    "For the BIC criterion, the best model is the one with the minimum BIC which is (again) the model (a). In this case, if we calculate the delta BICs of the other models with respect to the BIC* of the optimal model, they end up larger than 10. Taking into account the interpretation of the BIC criterion, there is very strong evidence that these candidate models are not the best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyNB1PBTLsAX"
   },
   "source": [
    "## Results and Comments for Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1641849165457,
     "user": {
      "displayName": "Valia Mavrikou",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjILpHRCIxbMLTJpbXrLUfXeR0D0fq59J9IzlIElg=s64",
      "userId": "01605552294531161702"
     },
     "user_tz": -120
    },
    "id": "c67DN8N-LvWh",
    "outputId": "c3db5d03-760f-4075-a4f2-356ae1123bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracies from Bayes Classifiers:\n",
      "\n",
      "Bayes classifier from 3a: 0.660304\n",
      "Bayes classifier from 3b: 0.671770\n",
      "Bayes classifier from 3c: 0.644651\n",
      "Bayes classifier from 3d: 0.529938\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracies from Bayes Classifiers:\\n\")\n",
    "    \n",
    "print(f\"Bayes classifier from 3a: {average_accuracy[0]:.6f}\")\n",
    "print(f\"Bayes classifier from 3b: {average_accuracy[1]:.6f}\")\n",
    "print(f\"Bayes classifier from 3c: {average_accuracy[2]:.6f}\")\n",
    "print(f\"Bayes classifier from 3d: {average_accuracy[3]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOVXHGvhAFhj"
   },
   "source": [
    "From the average results of predicting results on a test set (with k-fold validation) on the specific dataset, we find that the classifier from 3b outperforms the other 3 (not with a large margin from 3a and 3c). That leads us to the conclusion that the assumptions we made in 3b were more fitting to this dataset than the other 3. \n",
    "\n",
    "Comparing the results from all the Bayes classifiers and the knn method, it is obvious that the knn method gives better results than the classifiers. Generally speaking, the Bayes classifiers (and especially the Naive classifiers) are a good stepping stone to begin making predictions on a dataset and cannot be considered the optimal way of classification.\n",
    "\n",
    "However, we must make a special reference to the equiprobability of the two classes. The dataset is not split 50-50 between the two classes (500/268 samples for the two classes) so the property of equiprobability does not hold completely because it is most probable to chose class 0 than class 1 if we make a random selection from the dataset. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2.2_2.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
